import azure.functions as func
import logging
import os

app = func.FunctionApp()

@app.blob_trigger(arg_name="myblob", path="file-export/kafka/{name}",
                               connection="dwazstorageprod_STORAGE")
def SfBlobKafkaFunction(myblob: func.InputStream):
    logging.info(f"Python blob trigger function processed blob"
                f"Name: {myblob.name}"
                f"Blob Size: {myblob.length} bytes")
    print("---------START KAFKA-----------")
    # Installing 
    os.system(f'sudo pip install kafka-python')

    #Importing 
    from kafka import KafkaProducer
    kafka_conf = {
        'bootstrap_servers': 'pkc-ldvmy.centralus.azure.confluent.cloud:9092',
        'security_protocol': 'SASL_SSL',  # or 'SASL_PLAINTEXT' depending on your setup
        'sasl_mechanism': 'PLAIN',
        'sasl_plain_username': 'HP5AWQDHKBFZW35J',
        'sasl_plain_password': 'sbsiAqn15eysS2nN/5w2zYcI0sXhQMHnwr11MkePz9S1JzdYzFXuqodYGw8swfvB'
    }
    kafka_topic = 'snowflake_producer_poc'

    producer = KafkaProducer(
        bootstrap_servers=kafka_conf['bootstrap_servers'],
        security_protocol=kafka_conf['security_protocol'],
        sasl_mechanism=kafka_conf['sasl_mechanism'],
        sasl_plain_username=kafka_conf['sasl_plain_username'],
        sasl_plain_password=kafka_conf['sasl_plain_password'],
        value_serializer=lambda v: v.encode('utf-8'),
        key_serializer=lambda k: k.encode('utf-8')
    )
    
    # Read JSON data from blob
    blob_data = myblob.read().decode('utf-8')
    print("[DEBUG] blob data: ", blob_data)
    print("[DEBUG] blob data type: ", type(blob_data))

    # Send JSON data to Kafka
    try:
        producer.send(kafka_topic, key=myblob.name, value=blob_data).get(timeout=10)
        print(f"Successfully sent blob {myblob.name} to Kafka topic {kafka_topic}")
    except KafkaError as e:
        print(f"Failed to send blob {myblob.name} to Kafka: {e}")

    print("-------END-------")
